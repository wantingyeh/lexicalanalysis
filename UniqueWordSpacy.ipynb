{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4921d4dc-ff1a-4c41-a622-6e2a0e151c24",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "date: 08-11-2022 \\\n",
    "written by: Wan-Ting Yeh\n",
    "\n",
    "## purpose of the script:\n",
    "\n",
    "1. Walk through all the txt files in one folder\n",
    "2. use Spacy (NLP) to:\n",
    "    - clean the data (Exclude unwanted token, eg., punctuation)\n",
    "    - lemmentisation (talked, talking --> talk)\n",
    "    - custominsed lemmentisation (eg., peeeeeekaboo --> peekaboo)\n",
    "    - count unique word / total word\n",
    "    - list part of word (noun, pronoun, adj...)\n",
    "3. ouput file\n",
    "    - OUTPUT_PATH_final: unique word\n",
    "        - filename, unique words, total numbers of a file, total unique word counts, type-token ratio\n",
    "    - OUTPUT_PATH_pos: part of speech\n",
    "         - filename, original word, word lema, part of speech, explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d16b19-b2f9-4bfa-a62e-4c2bd3dff37c",
   "metadata": {},
   "source": [
    "### Step1: load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1f5f6f-c770-4157-81e0-31d5e2b2ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f012c-ec49-431b-9811-548a534c0450",
   "metadata": {},
   "source": [
    "### Step2: Specify constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf13388e-6dbb-4c47-bc7e-2fe5b8901241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FOLDER = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/\"\n",
    "OUTPUT_PATH_final = FOLDER + \"uniques_parent_UNIQUE_pooling_woint.csv\"\n",
    "OUTPUT_PATH_pos = FOLDER + \"uniques_parent_POS_pooling_woint.csv\"\n",
    "CONDITION = \"woint\"\n",
    "\n",
    "# final df output file format (count: unique words and total number words)\n",
    "columns_final = ['filename', 'unique_words', 'total_number_words', 'unique_word_count', 'TTR']\n",
    "final_df = pd.DataFrame(columns=columns_final)\n",
    "\n",
    "# part of speech output file format\n",
    "columns_pos = ['filename, word, word_lemma, word_pos, pos_explain']\n",
    "pos_df = pd.DataFrame(columns=columns_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a56db0-dd50-419a-8f5d-46b6ff150683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of txt files in the directory\n",
    "\n",
    "def list_text_files(folder, extension=\".txt\"):\n",
    "    text_file_paths = []\n",
    "    for root, dir, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                text_file_paths.append(os.path.join(root, file))\n",
    "    return text_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1fd91da-336a-4f64-b3fe-77c68d4eb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ['C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/0_NC_clean_pooling.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/1_C_clean_pooling.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/clean_all_text.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/CV-0_clean_overall_contigent.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/CV-1_clean_overall_contigent.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/CV_clean_pooling.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/DG-0_clean_overall_contigent.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/DG-1_clean_overall_contigent.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/DG_clean_pooling.txt']\n"
     ]
    }
   ],
   "source": [
    "# check the files\n",
    "\n",
    "text_file_paths = list_text_files(FOLDER)\n",
    "print(f\"path {text_file_paths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1003a4-98f1-4219-b984-031bedcfac2b",
   "metadata": {},
   "source": [
    "### Customised the lemmentisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61560513-7cd6-4c3f-9672-bf4f87cbc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "#syntax: ar.add([[{\"TEXT\":\"Bro\"}], [{\"TEXT\": \"Brah\"}]],{\"LEMMA\":\"brother\"})\n",
    "\n",
    "ar.add([[{\"TEXT\":\"let's\"}]],{\"LEMMA\":\"let us\"})\n",
    "ar.add([[{\"TEXT\":\"darle\"}], [{\"TEXT\":\"darling\"}]],{\"LEMMA\":\"darling\"})\n",
    "ar.add([[{\"TEXT\":\"is\"}]],{\"LEMMA\":\"be\"})\n",
    "ar.add([[{\"TEXT\":\"cant\"}]],{\"LEMMA\":\"can't\"})\n",
    "ar.add([[{\"TEXT\":\"aaahhh\"}], [{\"TEXT\": \"aaahh\"}], [{\"TEXT\": \"aaah\"}], [{\"TEXT\": \"ahhhh\"}], [{\"TEXT\": \"ahhh\"}], [{\"TEXT\": \"ahh\"}], [{\"TEXT\": \"aahh\"}],[{\"TEXT\": \"aah\"}], [{\"TEXT\": \"aaaa\"}], [{\"TEXT\": \"aaa\"}]], {\"LEMMA\":\"ah\"})\n",
    "ar.add([[{\"TEXT\":\"awhhhh\"}], [{\"TEXT\": \"awwww\"}], [{\"TEXT\": \"aww\"}], [{\"TEXT\": \"awww\"}],[{\"TEXT\": \"awh\"}]],{\"LEMMA\":\"aw\"})\n",
    "ar.add([[{\"TEXT\":\"ay\"}], [{\"TEXT\":\"ey\"}]],{\"LEMMA\":\"aye\"})\n",
    "ar.add([[{\"TEXT\":\"bahhh\"}], [{\"TEXT\":\"bahh\"}], [{\"TEXT\":\"bahhhh\"}]], {\"LEMMA\":\"bah\"})\n",
    "ar.add([[{\"TEXT\":\"boooeeehh\"}], [{\"TEXT\": \"booooo\"}], [{\"TEXT\": \"boooo\"}], [{\"TEXT\": \"booo\"}]],{\"LEMMA\":\"boo\"})\n",
    "ar.add([[{\"TEXT\":\"blahalalala\"}],[{\"TEXT\": \"blahhh\"}]],{\"LEMMA\":\"blabla\"})\n",
    "ar.add([[{\"TEXT\":\"bluey\"}]],{\"LEMMA\":\"blue\"})\n",
    "ar.add([[{\"TEXT\":\"baba\"}], [{\"TEXT\":\"bub\"}]],{\"LEMMA\":\"bubba\"})\n",
    "ar.add([[{\"TEXT\":\"child name\"}], [{\"TEXT\": \"childs name\"}], [{\"TEXT\": \"childsname\"}]],{\"LEMMA\":\"childname\"})\n",
    "ar.add([[{\"TEXT\":\"dada\"}], [{\"TEXT\": \"daddy\"}], [{\"TEXT\": \"dad\"}]],{\"LEMMA\":\"father\"})\n",
    "ar.add([[{\"TEXT\":\"doe\"}]],{\"LEMMA\":\"do\"})\n",
    "ar.add([[{\"TEXT\":\"doggy\"}], [{\"TEXT\": \"doggies\"}]],{\"LEMMA\":\"dog\"})\n",
    "ar.add([[{\"TEXT\":\"duckies\"}], [{\"TEXT\": \"ducky\"}]],{\"LEMMA\":\"duck\"})\n",
    "ar.add([[{\"TEXT\":\"e\"}], [{\"TEXT\": \"eee\"}]],{\"LEMMA\":\"ee\"})\n",
    "ar.add([[{\"TEXT\":\"gunna\"}], [{\"TEXT\": \"gonna\"}]],{\"LEMMA\":\"go to\"})\n",
    "ar.add([[{\"TEXT\":\"gonnne\"}], [{\"TEXT\": \"gooone\"}], [{\"TEXT\": \"gooo\"}], [{\"TEXT\": \"gone\"}], [{\"TEXT\": \"gon\"}], [{\"TEXT\": \"going\"}]],{\"LEMMA\":\"go\"})\n",
    "ar.add([[{\"TEXT\":\"hideee\"}], [{\"TEXT\": \"hidey\"}], [{\"TEXT\": \"hiding\"}]],{\"LEMMA\":\"hide\"})\n",
    "ar.add([[{\"TEXT\":\"hiiii\"}], [{\"TEXT\": \"hiii\"}], [{\"TEXT\": \"hii\"}], [{\"TEXT\": \"hai\"}]],{\"LEMMA\":\"hi\"})\n",
    "ar.add([[{\"TEXT\":\"heyyy\"}], [{\"TEXT\": \"heyy\"}]],{\"LEMMA\":\"hey\"})\n",
    "ar.add([[{\"TEXT\":\"hellooo\"}], [{\"TEXT\": \"helloo\"}]],{\"LEMMA\":\"hello\"})\n",
    "ar.add([[{\"TEXT\":\"jooob\"}]],{\"LEMMA\":\"job\"})\n",
    "ar.add([[{\"TEXT\":\"xxx\"}], [{\"TEXT\": \"xx\"}]],{\"LEMMA\":\"kiss\"})\n",
    "ar.add([[{\"TEXT\":\"meee\"}], [{\"TEXT\": \"mee\"}]],{\"LEMMA\":\"me\"})\n",
    "ar.add([[{\"TEXT\":\"mummyyyy\"}], [{\"TEXT\": \"mummyyy\"}], [{\"TEXT\": \"mummys\"}], [{\"TEXT\": \"ma\"}],[{\"TEXT\": \"muuummy\"}], [{\"TEXT\": \"x0019_mummy\"}], [{\"TEXT\": \"muummy\"}], [{\"TEXT\": \"mu\"}],[{\"TEXT\": \"mummy\"}], [{\"TEXT\": \"mama\"}], [{\"TEXT\": \"mama\"}], [{\"TEXT\": \"mumma\"}], [{\"TEXT\": \"muma\"}], [{\"TEXT\": \"mu-\"}], [{\"TEXT\": \"mum\"}]],{\"LEMMA\":\"mother\"})\n",
    "ar.add([[{\"TEXT\":\"mmmm\"}], [{\"TEXT\": \"mmm\"}], [{\"TEXT\":\"m.\"}],[{\"TEXT\":\"m\"}]],{\"LEMMA\":\"mm\"})\n",
    "ar.add([[{\"TEXT\":\"naaw\"}], [{\"TEXT\":\"naaaw\"}]],{\"LEMMA\":\"naw\"})\n",
    "ar.add([[{\"TEXT\":\"nooo\"}], [{\"TEXT\": \"nono\"}], [{\"TEXT\": \"noho\"}]],{\"LEMMA\":\"no\"})\n",
    "ar.add([[{\"TEXT\":\"ooooooooh\"}], [{\"TEXT\": \"ooh\"}], [{\"TEXT\": \"ohhh\"}], [{\"TEXT\": \"ohh\"}]],{\"LEMMA\":\"oh\"})\n",
    "ar.add([[{\"TEXT\":\"okay\"}],[{\"TEXT\":\"k\"}],[{\"TEXT\":\"ok\"}]], {\"LEMMA\":\"okay\"})\n",
    "ar.add([[{\"TEXT\":\"oooo\"}], [{\"TEXT\": \"ooo\"}]],{\"LEMMA\":\"oo\"})\n",
    "ar.add([[{\"TEXT\":\"oop\"}], [{\"TEXT\": \"ooph\"}], [{\"TEXT\": \"ooop\"}]],{\"LEMMA\":\"oops\"})\n",
    "ar.add([[{\"TEXT\":\"oyeee\"}]],{\"LEMMA\":\"oye\"})\n",
    "ar.add([[{\"TEXT\":\"peeee\"}], [{\"TEXT\": \"peeek\"}], [{\"TEXT\": \"peekk\"}], [{\"TEXT\": \"pee\"}]],{\"LEMMA\":\"peek\"})\n",
    "ar.add([[{\"TEXT\":\"peeeekaaa\"}], [{\"TEXT\": \"peeekaaa\"}], [{\"TEXT\": \"peekaaaa\"}],[{\"TEXT\": \"peeka-\"}], [{\"TEXT\": \"peekaaaaa\"}],[{\"TEXT\": \"peeekaaaa\"}],[{\"TEXT\": \"peekab\"}], [{\"TEXT\": \"peekaaa\"}], [{\"TEXT\": \"peakkaaaa\"}], [{\"TEXT\": \"peekkkaa\"}], [{\"TEXT\": \"peakkaa\"}], [{\"TEXT\": \"peekkaa\"}], [{\"TEXT\": \"peekkab\"}], [{\"TEXT\": \"peepo\"}], [{\"TEXT\": \"bebo\"}], [{\"TEXT\": \"peekka\"}]],{\"LEMMA\":\"peeka\"})\n",
    "ar.add([[{\"TEXT\":\"peekahboooo\"}], [{\"TEXT\": \"peekaboooo\"}], [{\"TEXT\": \"peekabooo\"}], [{\"TEXT\": \"peekkaboo\"}], [{\"TEXT\": \"peeaboo\"}], [{\"TEXT\": \"peek-a-boo\"}], [{\"TEXT\": \"peekaboo^\"}],[{\"TEXT\": \"peeakboo\"}], [{\"TEXT\": \"peekabooooo\"}]],{\"LEMMA\":\"peekaboo\"})\n",
    "ar.add([[{\"TEXT\":\"piggy\"}]],{\"LEMMA\":\"pig\"})\n",
    "ar.add([[{\"TEXT\":\"playng\"}], [{\"TEXT\": \"playin\"}],[{\"TEXT\": \"play-\"}]],{\"LEMMA\":\"play\"})\n",
    "ar.add([[{\"TEXT\":\"reaadyyyy\"}], [{\"TEXT\": \"reaaady\"}], [{\"TEXT\": \"readyyy\"}]],{\"LEMMA\":\"ready\"})\n",
    "ar.add([[{\"TEXT\":\"sheepy\"}]],{\"LEMMA\":\"sheep\"})\n",
    "ar.add([[{\"TEXT\":\"thereeee\"}], [{\"TEXT\": \"thereee\"}], [{\"TEXT\": \"theres\"}]],{\"LEMMA\":\"there\"})\n",
    "ar.add([[{\"TEXT\":\"tryna\"}], [{\"TEXT\":\"trynna\"}]],{\"LEMMA\":\"try\"})\n",
    "ar.add([[{\"TEXT\":\"wanna\"}]],{\"LEMMA\":\"want\"})\n",
    "ar.add([[{\"TEXT\":\"wewh\"}]],{\"LEMMA\":\"wew\"})\n",
    "ar.add([[{\"TEXT\":\"whats\"}], [{\"TEXT\":\"wha\"}]],{\"LEMMA\":\"what\"})\n",
    "ar.add([[{\"TEXT\":\"whereeeee\"}], [{\"TEXT\": \"wheeere\"}], [{\"TEXT\": \"whereee\"}], [{\"TEXT\": \"wheree\"}], [{\"TEXT\": \"wheres\"}]],{\"LEMMA\":\"where\"})\n",
    "ar.add([[{\"TEXT\":\"yayyyy\"}], [{\"TEXT\": \"yayyy\"}], [{\"TEXT\": \"yayy\"}], [{\"TEXT\": \"yyyay\"}]],{\"LEMMA\":\"yay\"})\n",
    "ar.add([[{\"TEXT\":\"yeahh\"}], [{\"TEXT\": \"yea\"}]],{\"LEMMA\":\"yeah\"})\n",
    "ar.add([[{\"TEXT\":\"youuuu\"}], [{\"TEXT\": \"youuu\"}]],{\"LEMMA\":\"you\"})\n",
    "ar.add([[{\"TEXT\":\"yoo\"}]],{\"LEMMA\":\"yo\"})\n",
    "ar.add([[{\"TEXT\":\"uhh\"}]],{\"LEMMA\":\"uh\"})\n",
    "ar.add([[{\"TEXT\":\"whoaa\"}]],{\"LEMMA\":\"whoa\"})\n",
    "ar.add([[{\"TEXT\":\"wooh\"}]],{\"LEMMA\":\"woo\"})\n",
    "ar.add([[{\"TEXT\":\"copter\"}]],{\"LEMMA\":\"helicopter\"})\n",
    "ar.add([[{\"TEXT\":\"ahhwhere\"}]],{\"LEMMA\":\"anywhere\"})\n",
    "ar.add([[{\"TEXT\":\"'ve\"}]],{\"LEMMA\":\" have\"})\n",
    "ar.add([[{\"TEXT\":\"'s\"}]],{\"LEMMA\":\" be\"})\n",
    "\n",
    "\n",
    "## manually seperate incorrect spacing words\n",
    "ar.add([[{\"TEXT\":\"whereâ€™swhere\"}]],{\"LEMMA\":\"where be where\"})\n",
    "ar.add([[{\"TEXT\":\"cancan\"}]],{\"LEMMA\":\"can can\"})\n",
    "ar.add([[{\"TEXT\":\"childname]peekaboo\"}]],{\"LEMMA\":\"childname peekaboo\"})\n",
    "ar.add([[{\"TEXT\":\"daddywhoo\"}]],{\"LEMMA\":\"dad who\"})\n",
    "ar.add([[{\"TEXT\":\"doingwe're\"}]],{\"LEMMA\":\"do we be\"})\n",
    "ar.add([[{\"TEXT\":\"ducksduck\"}]],{\"LEMMA\":\"duck duck\"})\n",
    "ar.add([[{\"TEXT\":\"onetwothree\"}]],{\"LEMMA\":\"one two three\"})\n",
    "ar.add([[{\"TEXT\":\"fourthreetwo\"}]],{\"LEMMA\":\"four three two\"})\n",
    "ar.add([[{\"TEXT\":\"iwe\"}]],{\"LEMMA\":\"i we\"})\n",
    "ar.add([[{\"TEXT\":\"mawhere\"}]],{\"LEMMA\":\"mum where\"})\n",
    "ar.add([[{\"TEXT\":\"wannado\"}]],{\"LEMMA\":\"want to do\"})\n",
    "ar.add([[{\"TEXT\":\"ta\"}], [{\"TEXT\":\"tha\"}]],{\"LEMMA\":\"that\"})\n",
    "ar.add([[{\"TEXT\":\"th\"}], [{\"TEXT\":\"ti\"}]],{\"LEMMA\":\"this\"})\n",
    "ar.add([[{\"TEXT\":\"youoh\"}]],{\"LEMMA\":\"you oh\"})\n",
    "ar.add([[{\"TEXT\":\"where'swhere\"}]],{\"LEMMA\":\"where be where\"})\n",
    "ar.add([[{\"TEXT\":\"hidecan\"}]],{\"LEMMA\":\"hide can\"})\n",
    "ar.add([[{\"TEXT\":\"youyou\"}]],{\"LEMMA\":\"you you\"})\n",
    "\n",
    "\n",
    "## manually exclude laugh and gasp\n",
    "ar.add([[{\"TEXT\":\"laughs\"}], [{\"TEXT\": \"laugh\"}], [{\"TEXT\": \"laughing\"}]],{\"LEMMA\":\"laugh\"})\n",
    "ar.add([[{\"TEXT\":\"gasps\"}], [{\"TEXT\": \"gasp\"}]],{\"LEMMA\":\"gasp\"})\n",
    "ar.add([[{\"TEXT\":\"*\"}], [{\"TEXT\": \"_\"}], [{\"TEXT\": \"]\"}], [{\"TEXT\": \"[\"}], [{\"TEXT\": \"_?\"}], [{\"TEXT\": \".\"}], [{\"TEXT\": \"s\"}]],{\"LEMMA\":\" \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a10a2-c889-486d-8897-ee9311fa4e4a",
   "metadata": {},
   "source": [
    "### Loop through file\n",
    "\n",
    "1. tokenise the data\n",
    "2. lemmentise data with customised words\n",
    "3. clean the data (no punctuation, space and \\n)\n",
    "4. **Exclude data**\n",
    "    - punctuation\n",
    "    - space\n",
    "    - X (other)\n",
    "    - symbol\n",
    "    - manual: manual_exlcude\n",
    "    - two version:\n",
    "        - exclude - interjection (eg., wow, whoa, yay)? [INTJ]\n",
    "        - non-exclusion for interjection (modify [manual_exclude])\n",
    "5. print out [word, word.lemma, word part of speech, explain part of speech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2273ae39-7441-4b3a-a38c-ed1e49f5f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually exclude words\n",
    "manual_exclude = [\"laugh\", \"gasp\", \"x0019_s\", \"x0014\", \"childname]_x0002\", \" \", \"\\x19s\", \"n\", \"?\", \"\\x14\", \n",
    "                  \"\\x19s-\", \"..\", \"!\", \"\\x19d\", \"\\x19re\", \"brr\", \"oi\", \"eh\", \"blabla\", \"dah\", \"woo\", \"oo\",\n",
    "                 \"waaa\", \"yo\", \"op\", \"whoopsie\", \"psst\", \"weee\", \"wew\", \"co\", \"aw\", \"rawr\", \"whoa\", \"oye\",\n",
    "                 \"hm\", \"dou\", \"aa\", \"hoo\", \"ewawa\", \"hmm\", \"buh\", \"tongtong\", \"ba\", \"whoo\", \"aye\", \"baa\",\n",
    "                 \"naw\", \"hmph\", \"shh\", \"titiro\", \"hoi\", \"eieio\", \"ohp\", \"mm\", \"heh\", \"weh\"]\n",
    "\n",
    "\n",
    "for file_path in text_file_paths:\n",
    "  with open(file_path, mode='r', encoding='cp1252') as text_file:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    text = text_file.read()\n",
    "\n",
    "    #cleaning texts\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = nlp(text)\n",
    "    \n",
    "    \n",
    "    filter_token = []          #filter_token: list of token after filtering lemma\n",
    "    token_pos = []             #token: part of word\n",
    "    unique = []                #unique: unique word\n",
    "    lines = []                 #line: a list of filename, word, word.lemma, word.pos, explanation\n",
    "    \n",
    "    for word in text:\n",
    "        if word.pos_ not in [\"SPACE\", \"PUNCT\", \"X\", \"SYM\", \"INTJ\"] and word.lemma_ not in manual_exclude:\n",
    "            filter_token.append(word.lemma_)\n",
    "            token_pos.append(word.pos_)\n",
    "            line = [file_name, word, word.lemma_, word.pos_, spacy.explain(word.pos_)]\n",
    "            lines.append(line)\n",
    "            \n",
    "            word_lemma = word.lemma_\n",
    "            \n",
    "#             print(word_lemma)\n",
    "#             save clean text into a text for frequency analysis for the future\n",
    "            with open(f\"{FOLDER}{file_name}_clean_{CONDITION}.txt\", mode=\"a\") as clean_file:\n",
    "                word_lemma = word_lemma.strip()\n",
    "                clean_file.write(f\"{word_lemma}\\n\")\n",
    "            \n",
    "            # unique token list\n",
    "            for token in filter_token:\n",
    "                if token not in unique:\n",
    "                    unique.append(token)\n",
    "    \n",
    "  \n",
    "    # numbers of words/ unique words\n",
    "    total_num = len(filter_token)\n",
    "    total_unique = len(unique)\n",
    "    \n",
    "    if total_unique == 0:\n",
    "        TTR = 0\n",
    "    else:\n",
    "        TTR = total_unique/total_num\n",
    "    \n",
    "    # save output for final df\n",
    "    data_row = pd.DataFrame(dict(zip(columns_final, [file_name, [unique], total_num, total_unique, TTR])))\n",
    "    final_df = pd.concat([final_df, data_row], axis=0, ignore_index=True) \n",
    "    \n",
    "    # save output for part of speech\n",
    "    data_row_pos = pd.DataFrame(dict(zip(columns_pos, [lines])))\n",
    "    pos_df  = pd.concat([pos_df, data_row_pos], axis=0, ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00249f1-a88f-43dd-8260-c80c8f2109d7",
   "metadata": {},
   "source": [
    "## save files\n",
    "\n",
    "1. final_df = unique word file\n",
    "    - filename, unique words, total numbers of a file, total unique word counts\n",
    "2. pos_df = part of speech file\n",
    "    - ['filename', 'original_word', 'word_lemma', 'word_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6649f4-99a3-4a84-9dc0-8ea16658d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'be', 'go', 'play', 'peekaboo', 'alright', 'this', 'we', 'to', 'where', 'mother', 'boo', 'you', 'want', 'look', 'in', 'behind', 'here', 'well', 'he', 'childname', 'there', 'she', 'I', 'have', 'get', 'the', 'duck', 'sorry', 'darling', 'change', 'do', 'with', 'my', 'face', 'can', 'see', 'animal', 'no', 'but', 'what', 'just', 'and', 'like', 'a', 'magician', 'make', 'they', 'disappear', 'know', 'say', 'remember', 'how', 'people', 'ready', 'cover', 'count', 'again', 'your', 'eye', 'one', 'two', 'now', 'try', 'that', 'okay', 'time', 'three', 'peeka', 'hide', 'hang', 'on', 'drop', 'helicopter', 'at', 'turn', 'should', 'new', 'yourself', 'thing', 'let', 'around', 'gosh', 'leave', 'screen', 'so', 'many', 'interesting', 'for', 'nose', 'would', 'curtain', 'goat', 'about', 'nah', 'cow', 'farm', 'later', 'back', 'not', 'out', 'come', 'nothing', 'yay', 'sit', 'baby', 'hand', 'bubba', 'will', 'bah', 'pardon', 'bag', 'yep', 'alone', 'could', 'must', 'her', 'hello', 'sweetheart', 'show', 'right', 'dog', 'bye', 'sleep', 'shall', 'why', 'funny', 'game', 'way', 'away', 'nightie', 'chicken', 'next', 'off', 'yet', 'put', 'down', 'finish', 'sheep', 'little', 'swim', 'computer', 'too', 'pig', 'peek', 'all', 'through', 'door', 'hi', 'happen', 'father', 'some', 'over', 'good', 'boy', 'steady', 'hold', 'task', 'stay', 'though', 'bottom', 'oh', 'careful', 'morning', 'gentle', 'tablet', 'bore', 'if', 'mouse', 'break', 'proof', 'oop', 'bit', 'sick', 'of', 'chair', 'cuddle', 'ha', 'fun', 'friend', 'point', 'blicket', 'ee', 'tv', 'still', 'hey', 'find', 'sigh', 'actually', 'forward', 'think', 'touch', 'cause', 'up', 'because', 'mean', 'more', 'as', 'weh', 'block', 'job', 'wait', 'enjoy', 'use', 'light', 'give', 'fright', 'love', 'push', 'even', 'place', 'need', 'lift', 'top', 'shark', 'kiss', 'trick', 'girl', 'minute', 'naughty', 'its', 'reflection', 'much', 'black', 'bin', 'cheese', 'another', 'seat', 'child', 'name', 'who', 'first', 'or', 'tower', 'rocketship', 'after', 'then', 'tiger', 'stuff', 'might', 'something', 'different', 'house', 'idea', 'press', 'button', 'open', 'shut', 'water', 'tear', 'scream', 'tickle', 'buddy', 'cloud', 'touching', 'everything', 'before', 'when', 'enough', 'thank', 'office', 'work', 'farmer', 'last', 'cute', 'cheeky', 'chop', 'giggle', 'soon', 'very', 'quick', 'blue', 'hit', 'watch', 'together', 'cross', 'toy', 'nicely', 'listening', 'listen', 'four', 'entertain', 'kick', 'magic', 'pakarei', 'anything', 'hot', 'chin', 'sing', 'anymore', 'camera', 'bottle']\n"
     ]
    }
   ],
   "source": [
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fedba85-c283-4f13-87b8-ab26ab593584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>total_number_words</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_NC_clean_pooling.txt</td>\n",
       "      <td>[it, be, go, play, peekaboo, alright, this, we...</td>\n",
       "      <td>4541</td>\n",
       "      <td>233</td>\n",
       "      <td>0.051310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_C_clean_pooling.txt</td>\n",
       "      <td>[where, be, mother, I, have, get, the, duck, s...</td>\n",
       "      <td>2245</td>\n",
       "      <td>197</td>\n",
       "      <td>0.087751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean_all_text.txt</td>\n",
       "      <td>[it, be, go, play, peekaboo, alright, this, we...</td>\n",
       "      <td>6816</td>\n",
       "      <td>291</td>\n",
       "      <td>0.042694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CV-0_clean_overall_contigent.txt</td>\n",
       "      <td>[it, be, go, play, peekaboo, alright, this, we...</td>\n",
       "      <td>2390</td>\n",
       "      <td>196</td>\n",
       "      <td>0.082008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CV-1_clean_overall_contigent.txt</td>\n",
       "      <td>[where, be, mother, I, have, get, the, duck, s...</td>\n",
       "      <td>1335</td>\n",
       "      <td>158</td>\n",
       "      <td>0.118352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0            0_NC_clean_pooling.txt   \n",
       "1             1_C_clean_pooling.txt   \n",
       "2                clean_all_text.txt   \n",
       "3  CV-0_clean_overall_contigent.txt   \n",
       "4  CV-1_clean_overall_contigent.txt   \n",
       "\n",
       "                                        unique_words total_number_words  \\\n",
       "0  [it, be, go, play, peekaboo, alright, this, we...               4541   \n",
       "1  [where, be, mother, I, have, get, the, duck, s...               2245   \n",
       "2  [it, be, go, play, peekaboo, alright, this, we...               6816   \n",
       "3  [it, be, go, play, peekaboo, alright, this, we...               2390   \n",
       "4  [where, be, mother, I, have, get, the, duck, s...               1335   \n",
       "\n",
       "  unique_word_count       TTR  \n",
       "0               233  0.051310  \n",
       "1               197  0.087751  \n",
       "2               291  0.042694  \n",
       "3               196  0.082008  \n",
       "4               158  0.118352  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_num)\n",
    "final_df.to_csv(OUTPUT_PATH_final)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d612734b-9f09-4ef3-bfc5-f3c74b5a48ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename, word, word_lemma, word_pos, pos_explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0_NC_clean_pooling.txt, it, it, PRON, pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0_NC_clean_pooling.txt, be, be, AUX, auxiliary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0_NC_clean_pooling.txt, go, go, VERB, verb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0_NC_clean_pooling.txt, play, play, VERB, verb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0_NC_clean_pooling.txt, peekaboo, peekaboo, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename, word, word_lemma, word_pos, pos_explain\n",
       "0    [0_NC_clean_pooling.txt, it, it, PRON, pronoun]\n",
       "1   [0_NC_clean_pooling.txt, be, be, AUX, auxiliary]\n",
       "2       [0_NC_clean_pooling.txt, go, go, VERB, verb]\n",
       "3   [0_NC_clean_pooling.txt, play, play, VERB, verb]\n",
       "4  [0_NC_clean_pooling.txt, peekaboo, peekaboo, N..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(token_pos)\n",
    "pos_df.to_csv(OUTPUT_PATH_pos)\n",
    "pos_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
