{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f7fefd-a5a7-427c-9136-5cea99c8adba",
   "metadata": {},
   "source": [
    "# Random sampling\n",
    "\n",
    "purpose: random sampling the word tokens from the corpora, so we have the equal sampling size \\\n",
    "Looping through different sample sizes: \\\n",
    "from 100 to 2000, with a step = 100 \\\n",
    "so, we can have varying sample sizes for different conditions/groups\n",
    "\n",
    "## with interjections\n",
    "\n",
    "from 100 to 2000, with a step = 100 \n",
    "- OVERALL_TXT = \"clean_parent_all.txt\" (total words: 7296)\n",
    "- CV_TXT = \"CV_clean_overall_condition.txt\" (total words: 3992)\n",
    "- DG_TXT = \"DG_clean_overall_condition.txt\" (total words: 3279)\n",
    "- CONTI_TXT = \"contingency_clean_overall_contingency.txt\" (total words: 2424)\n",
    "- NONCON_TXT = \"non-contingency_clean_overall_contingency.txt\"  (total words: 4845) \n",
    "\n",
    "\n",
    "from 100 to 900, with a step = 100 \n",
    "- CV_C_TXT = \"CV-1_clean_overall_contigent\" (total words: 2558)\n",
    "- CV_NC_TXT = \"CV-0_clean_overall_Ncontigent\" (total words: 1434) \n",
    "- DG_C_TXT = \"DG-1_clean_overall_contigent\" (total words: 2289)\n",
    "- DG_NC_TXT = \"DG-0_clean_overall_Ncontigent\" (total words: 990)\n",
    "\n",
    "## without interjections & sound-like words\n",
    "\n",
    "- OVERALL_TXT = \"clean_pooling_all_clean_woint.txt\" (total words: 6816)\n",
    "- CV_TXT = \"CV_pooling_clean_woint.txt\" (total words: 3725)\n",
    "- DG_TXT = \"DG_pooling_clean_woint.txt\" (total words: 3065)\n",
    "- CONTI_TXT = \"1_C_pooling_clean_woint.txt\" (total words: 2245)\n",
    "- NONCON_TXT = \"0_NC_pooling_clean_woint.txt\"  (total words: 4541) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f693d9f-cf50-4a5e-9f00-e0702610fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37149328-fe25-40a4-bee7-777e635cd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chunck is for with interjections\n",
    "# FOLDER_PATH = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/clean_data\"\n",
    "# OVERALL_TXT = \"clean_all_text.txt\"\n",
    "# CV_TXT = \"CV_clean_pooling.txt\"\n",
    "# DG_TXT = \"DG_clean_pooling.txt\"\n",
    "# CONTI_TXT = \"1_C_clean_pooling.txt\"\n",
    "# NONCON_TXT = \"0_NC_clean_pooling.txt\"\n",
    "# CV_C_TXT = \"CV-1_clean_overall_contigent.txt\" \n",
    "# CV_NC_TXT = \"CV-0_clean_overall_Ncontigent.txt\" \n",
    "# DG_C_TXT = \"DG-1_clean_overall_contigent.txt\" \n",
    "# DG_NC_TXT = \"DG-0_clean_overall_Ncontigent.txt\" \n",
    "# OUTPUT_PATH = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/output/inc_interjections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2805bae0-c485-46f6-ad9d-eae8863dbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is for without interjections\n",
    "FOLDER_PATH = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections\"\n",
    "OVERALL_TXT = \"clean_pooling_all_clean_woint.txt\"\n",
    "\n",
    "# condition\n",
    "CV_TXT = \"CV_pooling_clean_woint.txt\"\n",
    "DG_TXT = \"DG_pooling_clean_woint.txt\"\n",
    "\n",
    "# speech type\n",
    "CONTI_TXT = \"1_C_pooling_clean_woint.txt\"\n",
    "NONCON_TXT = \"0_NC_pooling_clean_woint.txt\"\n",
    "\n",
    "# condition x speech type\n",
    "CV_C_TXT = \"CV-1_pooling_clean_woint.txt\" \n",
    "CV_NC_TXT = \"CV-0_pooling_clean_woint.txt\" \n",
    "DG_C_TXT = \"DG-1_pooling_clean_woint.txt\" \n",
    "DG_NC_TXT = \"DG-0_pooling_clean_woint.txt\" \n",
    "\n",
    "OUTPUT_PATH = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/output/wo_interjections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d3d7b61a-f1ea-4b62-ad29-d1f68441a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_CON = NONCON_TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "185e2f90-9867-4ebb-a89b-3c2c77b1021f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/USER/PycharmProjects/UniqueWordCalculator/parent-condition/pooling-data/wo_interjections/0_NC_pooling_clean_woint.txt'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILENAME = FOLDER_PATH + \"/\" + CURRENT_CON\n",
    "FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3c9c66a-3b26-4dda-9fdc-a66f05b2e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME) as all_file:\n",
    "    all_content = all_file.readlines()\n",
    "#     all_content = [token.strip('\\n') for token in all_content]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067374b-2222-44e7-a1f1-db1f8a868b0b",
   "metadata": {},
   "source": [
    "# iteration for  3 times\n",
    "\n",
    "1. loop from starting size to end size with steps\n",
    "2. repeat this method for 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2a69c0d-7f0b-4dd3-931e-78305e147e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_SIZE = 2000\n",
    "ENDING_SIZE = 2000\n",
    "STEP = 1\n",
    "ITER_COUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1fe984eb-4c28-408f-a54f-06b9e64dd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in DataFrame\n",
    "columns = ['sample_size', 'iteration', 'word', 'rank', 'frequency']\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1793dac-1d9d-46f8-85bb-6fd96c6e4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating for 50 times\n",
    "for iter_num in range(1, ITER_COUNT + 1):\n",
    "    # Randomly sampling from 100 to 2000, with step 100\n",
    "    for sample_size in range(STARTING_SIZE, ENDING_SIZE + STEP, STEP):\n",
    "        sampled_words = random.sample(all_content, sample_size)\n",
    "        \n",
    "        # Count the frequency of words\n",
    "        word_freq = Counter(sampled_words)\n",
    "        \n",
    "        # Sort by frequency and assign rank\n",
    "        ranked_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Append results to DataFrame\n",
    "        for rank, (word, freq) in enumerate(ranked_words, start=1):\n",
    "            results = results.append({'sample_size': sample_size, 'iteration': iter_num,\n",
    "                                       'word': word, 'rank': rank, 'frequency': freq}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a10e615-5bce-44ec-86d1-32f7324c146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sample_size iteration        word rank frequency\n",
      "0          2000         1        be\\n    1       268\n",
      "1          2000         1  peekaboo\\n    2       183\n",
      "2          2000         1     where\\n    3       180\n",
      "3          2000         1       you\\n    4       112\n",
      "4          2000         1       boo\\n    5       103\n",
      "..          ...       ...         ...  ...       ...\n",
      "499        2000         3      need\\n  163         1\n",
      "500        2000         3     watch\\n  164         1\n",
      "501        2000         3      sing\\n  165         1\n",
      "502        2000         3      sigh\\n  166         1\n",
      "503        2000         3     child\\n  167         1\n",
      "\n",
      "[504 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "\n",
    "outputfile = f'output_frequrank_{CURRENT_CON}_it03.csv'\n",
    "results.to_csv(outputfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d885f-9f03-4e08-9758-1c2fbff9c949",
   "metadata": {},
   "source": [
    "# Speech type x condition\n",
    "\n",
    "1. starting from 450 to 900\n",
    "2. repeat 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "14e35a7d-10aa-4a98-af3f-f7442bc49c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_SIZE = 450\n",
    "ENDING_SIZE = 900\n",
    "STEP = 450\n",
    "ITER_COUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e43fd3aa-e732-456e-9cf3-4c355f5da763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in DataFrame\n",
    "columns = ['sample_size', 'iteration', 'word', 'rank', 'frequency']\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c4c88b7a-f222-44a9-934d-6d318a0a97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating for 4 times\n",
    "for iter_num in range(1, ITER_COUNT + 1):\n",
    "    # Randomly sampling from 100 to 2000, with step 100\n",
    "    for sample_size in range(STARTING_SIZE, ENDING_SIZE + STEP, STEP):\n",
    "        sampled_words = random.sample(all_content, sample_size)\n",
    "        \n",
    "        # Count the frequency of words\n",
    "        word_freq = Counter(sampled_words)\n",
    "        \n",
    "        # Sort by frequency and assign rank\n",
    "        ranked_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Append results to DataFrame\n",
    "        for rank, (word, freq) in enumerate(ranked_words, start=1):\n",
    "            results = results.append({'sample_size': sample_size, 'iteration': iter_num,\n",
    "                                       'word': word, 'rank': rank, 'frequency': freq}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e850a2f3-0a3e-4012-b383-cce29595610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sample_size iteration        word rank frequency\n",
      "0           450         1        be\\n    1        61\n",
      "1           450         1  peekaboo\\n    2        35\n",
      "2           450         1     where\\n    3        34\n",
      "3           450         1       boo\\n    4        32\n",
      "4           450         1       you\\n    5        27\n",
      "..          ...       ...         ...  ...       ...\n",
      "759         900         4     leave\\n  106         1\n",
      "760         900         4      baby\\n  107         1\n",
      "761         900         4    little\\n  108         1\n",
      "762         900         4      know\\n  109         1\n",
      "763         900         4       boy\\n  110         1\n",
      "\n",
      "[764 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "\n",
    "outputfile = f'output_frequrank_{CURRENT_CON}.csv'\n",
    "results.to_csv(outputfile, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
